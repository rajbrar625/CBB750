{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Section 1 - Parts a and b: Classification using NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim: 500 rows and 8021 columns \n",
      "\"Easy\" Data: Mislabeled Points: 36 out of 125\n",
      "Dim: 500 rows and 10000 columns \n",
      "\"Hard\" Data: Mislabeled Points: 41 out of 125\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd  \n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB as NB\n",
    "from sklearn import tree\n",
    "\n",
    "# \"Easy\" Data\n",
    "cat = ['comp.sys.ibm.pc.hardware',\n",
    " 'comp.sys.mac.hardware','rec.sport.baseball',\n",
    " 'rec.sport.hockey']\n",
    "news = fetch_20newsgroups(subset='train',\n",
    "                          categories=cat,\n",
    "                          remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    " \n",
    "term_freq = CountVectorizer (max_df=500, \n",
    "                      min_df=0,\n",
    "                      max_features =10000, \n",
    "                      ngram_range =(1,1),\n",
    "                     stop_words='english')\n",
    "tf_matrix=term_freq.fit_transform(news.data[:500])   \n",
    "print (\"Dim: %d rows and %d columns \" % (tf_matrix.shape[0], tf_matrix.shape[1]))\n",
    "\n",
    "            \n",
    "full_matrix = pd.DataFrame(tf_matrix.todense(),columns=term_freq.get_feature_names())\n",
    " \n",
    "\n",
    "t=np.asarray(news.target[:500])    \n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(full_matrix.as_matrix(),t,random_state=50) \n",
    "\n",
    "\n",
    "clf = NB()\n",
    "#from sklearn import tree\n",
    "#clf = tree.DecisionTreeClassifier()\n",
    "y_pred = clf.fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"\\\"Easy\\\" Data: Mislabeled Points: %d out of %d\" % (error,xtest.shape[0] ))\n",
    "\n",
    "\n",
    "# \"Hard\" Data\n",
    "cat = ['sci.space', 'rec.motorcycles',\n",
    " 'rec.autos']\n",
    "news2 = fetch_20newsgroups(subset='train',\n",
    "                          categories=cat,\n",
    "                          remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    " \n",
    "term_freq = CountVectorizer (max_df=500, \n",
    "                      min_df=0,\n",
    "                      max_features =10000, \n",
    "                      ngram_range =(1,1),\n",
    "                     stop_words='english')\n",
    "tf_matrix=term_freq.fit_transform(news2.data[:500])   \n",
    "print (\"Dim: %d rows and %d columns \" % (tf_matrix.shape[0], tf_matrix.shape[1]))\n",
    "\n",
    "              \n",
    "full_matrix = pd.DataFrame(tf_matrix.todense(),columns=term_freq.get_feature_names())\n",
    " \n",
    "t=np.asarray(news2.target[:500])    \n",
    " \n",
    "xtrain, xtest, ytrain, ytest = train_test_split(full_matrix.as_matrix(),t,random_state=50) \n",
    "\n",
    " \n",
    "clf = NB()\n",
    "#from sklearn import tree\n",
    "#clf = tree.DecisionTreeClassifier()\n",
    "y_pred = clf.fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"\\\"Hard\\\" Data: Mislabeled Points: %d out of %d\" % (error,xtest.shape[0] ))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Section 1 - Part c: Classification using the Decison Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim: 500 rows and 8021 columns \n",
      "\"Easy\" Data: Mislabeled Points: 43 out of 125\n",
      "Dim: 500 rows and 10000 columns \n",
      "\"Hard\" Data: Mislabeled Points: 54 out of 125\n"
     ]
    }
   ],
   "source": [
    "# \"Easy\" Data\n",
    "\n",
    "term_freq = CountVectorizer (max_df=500, \n",
    "                      min_df=0,\n",
    "                      max_features =10000, \n",
    "                      ngram_range =(1,1),\n",
    "                     stop_words='english')\n",
    "tf_matrix=term_freq.fit_transform(news.data[:500])   \n",
    "print (\"Dim: %d rows and %d columns \" % (tf_matrix.shape[0], tf_matrix.shape[1]))\n",
    "            \n",
    "full_matrix = pd.DataFrame(tf_matrix.todense(),columns=term_freq.get_feature_names())\n",
    " \n",
    "t=np.asarray(news.target[:500])    \n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(full_matrix.as_matrix(),t,random_state=50) \n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "y_pred = clf.fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"\\\"Easy\\\" Data: Mislabeled Points: %d out of %d\" % (error,xtest.shape[0] ))\n",
    "\n",
    "\n",
    "# \"Hard\" Data\n",
    " \n",
    "term_freq = CountVectorizer (max_df=500, \n",
    "                      min_df=0,\n",
    "                      max_features =10000, \n",
    "                      ngram_range =(1,1),\n",
    "                     stop_words='english')\n",
    "tf_matrix=term_freq.fit_transform(news2.data[:500])   \n",
    "print (\"Dim: %d rows and %d columns \" % (tf_matrix.shape[0], tf_matrix.shape[1]))\n",
    "\n",
    "              \n",
    "full_matrix = pd.DataFrame(tf_matrix.todense(),columns=term_freq.get_feature_names())\n",
    " \n",
    "t=np.asarray(news2.target[:500])    \n",
    " \n",
    "xtrain, xtest, ytrain, ytest = train_test_split(full_matrix.as_matrix(),t,random_state=50) \n",
    "\n",
    " \n",
    "clf = tree.DecisionTreeClassifier()\n",
    "y_pred = clf.fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"\\\"Hard\\\" Data: Mislabeled Points: %d out of %d\" % (error,xtest.shape[0] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Section 1 - Part d - Discussion: The NB classifier performed better than the Decision Tree classifier in cases \n",
    "## of \"easy\" and \"hard\" data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Section 2 - Parts a,b, and c: Classification using Ensemble methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Bagging - Mislabeled Points: 36 out of 125\n",
      "With AdaBoost - Mislabeled Points: 41 out of 125\n",
      "With Random Forest- Mislabeled Points: 39 out of 125\n"
     ]
    }
   ],
   "source": [
    "# Using \"Hard\" data from above\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(full_matrix.as_matrix(),t,random_state=50)\n",
    "\n",
    "bagging = BaggingClassifier (NB(), max_samples=.5, max_features=.5)\n",
    "y_pred = bagging.fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"With Bagging - Mislabeled Points: %d out of %d\" % (error,xtest.shape[0] ))\n",
    "\n",
    " \n",
    "clf = AdaBoostClassifier(NB(),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=300)\n",
    "\n",
    "y_pred = clf.fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"With AdaBoost - Mislabeled Points: %d out of %d\" % (error,xtest.shape[0]))\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=None,random_state=10, max_features='auto')\n",
    "y_pred = clf.fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"With Random Forest- Mislabeled Points: %d out of %d\" % (error,xtest.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bagging: Average Accuracy with 10 folds is 0.760\n",
      " Bagging: Average Accuracy with 20 folds is 0.772\n",
      " Bagging: Average Accuracy with 30 folds is 0.739\n"
     ]
    }
   ],
   "source": [
    "# k-folds with bagging\n",
    "E1=[]\n",
    "n_splits=10\n",
    "\n",
    "kf = KFold(n_splits)\n",
    " \n",
    "for train, test in kf.split(tf_matrix): \n",
    "    xtrain,xtest = tf_matrix[train],  tf_matrix[test]\n",
    "    ytrain, ytest = t[train], t[test]\n",
    "    clf = BaggingClassifier (NB(), max_samples=.5, max_features=.5)\n",
    "    y = clf.fit(xtrain.toarray(), ytrain).predict(xtest.toarray())\n",
    "    acc=metrics.accuracy_score(ytest, y)\n",
    "    E1.append(acc)\n",
    "    \n",
    "avg_accuracy = sum(E1)/n_splits\n",
    "print (\" Bagging: Average Accuracy with %d folds is %3.3f\" % (n_splits,avg_accuracy))\n",
    "\n",
    "\n",
    "E2 = []\n",
    "n_splits=20\n",
    "\n",
    "kf = KFold(n_splits)\n",
    " \n",
    "for train, test in kf.split(tf_matrix): \n",
    "    xtrain,xtest = tf_matrix[train],  tf_matrix[test]\n",
    "    ytrain, ytest = t[train], t[test]\n",
    "    clf = BaggingClassifier (NB(), max_samples=.5, max_features=.5)\n",
    "    y = clf.fit(xtrain.toarray(), ytrain).predict(xtest.toarray())\n",
    "    acc=metrics.accuracy_score(ytest, y)\n",
    "    E2.append(acc)\n",
    "    \n",
    "avg_accuracy = sum(E2)/n_splits\n",
    "print (\" Bagging: Average Accuracy with %d folds is %3.3f\" % (n_splits,avg_accuracy))\n",
    "\n",
    "E3 = []\n",
    "n_splits=30\n",
    "\n",
    "kf = KFold(n_splits)\n",
    " \n",
    "for train, test in kf.split(tf_matrix): \n",
    "    xtrain,xtest = tf_matrix[train],  tf_matrix[test]\n",
    "    ytrain, ytest = t[train], t[test]\n",
    "    clf = BaggingClassifier (NB(), max_samples=.5, max_features=.5)\n",
    "    y = clf.fit(xtrain.toarray(), ytrain).predict(xtest.toarray())\n",
    "    acc=metrics.accuracy_score(ytest, y)\n",
    "    E3.append(acc)\n",
    "    \n",
    "avg_accuracy = sum(E3)/n_splits\n",
    "print (\" Bagging: Average Accuracy with %d folds is %3.3f\" % (n_splits,avg_accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Adaboost: Average Accuracy with 10 folds is 0.790\n",
      " Adaboost: Average Accuracy with 20 folds is 0.792\n",
      " Adaboost: Average Accuracy with 30 folds is 0.786\n"
     ]
    }
   ],
   "source": [
    " # k-folds with Adaboost\n",
    "E4 = []\n",
    "n_splits=10\n",
    "\n",
    "kf = KFold(n_splits)\n",
    " \n",
    "for train, test in kf.split(tf_matrix): \n",
    "    xtrain,xtest = tf_matrix[train],  tf_matrix[test]\n",
    "    ytrain, ytest = t[train], t[test]\n",
    "    clf = AdaBoostClassifier(NB(),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=300)\n",
    "    y = clf.fit(xtrain.toarray(), ytrain).predict(xtest.toarray())\n",
    "    acc=metrics.accuracy_score(ytest, y)\n",
    "    E4.append(acc)\n",
    "    \n",
    "avg_accuracy = sum(E4)/n_splits\n",
    "print (\" Adaboost: Average Accuracy with %d folds is %3.3f\" % (n_splits,avg_accuracy))\n",
    "\n",
    "\n",
    "E5=[]\n",
    "n_splits=20\n",
    "\n",
    "kf = KFold(n_splits)\n",
    " \n",
    "for train, test in kf.split(tf_matrix): \n",
    "    xtrain,xtest = tf_matrix[train],  tf_matrix[test]\n",
    "    ytrain, ytest = t[train], t[test]\n",
    "    clf = AdaBoostClassifier(NB(),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=300)\n",
    "    y = clf.fit(xtrain.toarray(), ytrain).predict(xtest.toarray())\n",
    "    acc=metrics.accuracy_score(ytest, y)\n",
    "    E5.append(acc)\n",
    "    \n",
    "avg_accuracy = sum(E5)/n_splits\n",
    "print (\" Adaboost: Average Accuracy with %d folds is %3.3f\" % (n_splits,avg_accuracy))\n",
    "\n",
    "E6=[]\n",
    "n_splits=30\n",
    "\n",
    "kf = KFold(n_splits)\n",
    " \n",
    "for train, test in kf.split(tf_matrix): \n",
    "    xtrain,xtest = tf_matrix[train],  tf_matrix[test]\n",
    "    ytrain, ytest = t[train], t[test]\n",
    "    clf = AdaBoostClassifier(NB(),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=300)\n",
    "    y = clf.fit(xtrain.toarray(), ytrain).predict(xtest.toarray())\n",
    "    acc=metrics.accuracy_score(ytest, y)\n",
    "    E6.append(acc)\n",
    "    \n",
    "avg_accuracy = sum(E6)/n_splits\n",
    "print (\" Adaboost: Average Accuracy with %d folds is %3.3f\" % (n_splits,avg_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Randomforest: Average Accuracy with 10 folds is 0.746\n",
      " Randomforest: Average Accuracy with 20 folds is 0.764\n",
      " Randomforest: Average Accuracy with 30 folds is 0.766\n"
     ]
    }
   ],
   "source": [
    "# k-folds with Randomforests\n",
    "E7 = []\n",
    "n_splits=10\n",
    "\n",
    "kf = KFold(n_splits)\n",
    " \n",
    "for train, test in kf.split(tf_matrix): \n",
    "    xtrain,xtest = tf_matrix[train],  tf_matrix[test]\n",
    "    ytrain, ytest = t[train], t[test]\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=None,random_state=10, max_features='auto')\n",
    "    y = clf.fit(xtrain.toarray(), ytrain).predict(xtest.toarray())\n",
    "    acc=metrics.accuracy_score(ytest, y)\n",
    "    E7.append(acc)\n",
    "    \n",
    "avg_accuracy = sum(E7)/n_splits\n",
    "print (\" Randomforest: Average Accuracy with %d folds is %3.3f\" % (n_splits,avg_accuracy))\n",
    "\n",
    "\n",
    "E8=[]\n",
    "n_splits=20\n",
    "\n",
    "kf = KFold(n_splits)\n",
    " \n",
    "for train, test in kf.split(tf_matrix): \n",
    "    xtrain,xtest = tf_matrix[train],  tf_matrix[test]\n",
    "    ytrain, ytest = t[train], t[test]\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=None,random_state=10, max_features='auto')\n",
    "    y = clf.fit(xtrain.toarray(), ytrain).predict(xtest.toarray())\n",
    "    acc=metrics.accuracy_score(ytest, y)\n",
    "    E8.append(acc)\n",
    "    \n",
    "avg_accuracy = sum(E8)/n_splits\n",
    "print (\" Randomforest: Average Accuracy with %d folds is %3.3f\" % (n_splits,avg_accuracy))\n",
    "\n",
    "E9=[]\n",
    "n_splits=30\n",
    "\n",
    "kf = KFold(n_splits)\n",
    " \n",
    "for train, test in kf.split(tf_matrix): \n",
    "    xtrain,xtest = tf_matrix[train],  tf_matrix[test]\n",
    "    ytrain, ytest = t[train], t[test]\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=None,random_state=10, max_features='auto')\n",
    "    y = clf.fit(xtrain.toarray(), ytrain).predict(xtest.toarray())\n",
    "    acc=metrics.accuracy_score(ytest, y)\n",
    "    E9.append(acc)\n",
    "    \n",
    "avg_accuracy = sum(E9)/n_splits\n",
    "print (\" Randomforest: Average Accuracy with %d folds is %3.3f\" % (n_splits,avg_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Discussion: Adaboost gave the best performance (based on accuracy).  Changing the number of folds resulted in relatively stable performance.  The largest gain in accuracy was achieved by Bagging, whereas the accuracy of adaboost decreased when going from 10 to 30 folds (with a slight increase between 10 and 20).\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Section 3: Multi-label Classification   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim: 2323 rows and 29847 columns  \n",
      "One-vs-All: SVC --> Mislabeled Points: 92 out of 581\n",
      "One-vs-All: NB --> Mislabeled Points: 125 out of 581\n",
      "One-vs-All: DecisionTree --> Mislabeled Points: 116 out of 581\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# One-vs-all classification\n",
    "\n",
    "news = fetch_20newsgroups(subset='train',\n",
    "                          categories=('comp.graphics', 'rec.autos', 'talk.politics.guns', 'soc.religion.christian'),\n",
    "                          remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "#generate term frequency matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tf_vec = CountVectorizer (max_df=500, \n",
    "                      min_df=0,\n",
    "                      max_features =30000, \n",
    "                      ngram_range =(1,1),\n",
    "                     stop_words='english')\n",
    "\n",
    "tf_matrix=tf_vec.fit_transform(news.data)  #sparse matrix\n",
    "print (\"Dim: %d rows and %d columns  \" % (tf_matrix.shape[0], tf_matrix.shape[1]))\n",
    "\n",
    "full_matrix = pd.DataFrame(tf_matrix.todense(),columns=tf_vec.get_feature_names())\n",
    "\n",
    "\n",
    "#One-vs-All (one-vs-Rest)\n",
    " \n",
    "t=np.asarray(news.target)   # true labels\n",
    " \n",
    "xtrain, xtest, ytrain, ytest = train_test_split(full_matrix.as_matrix(),t,random_state=50) \n",
    "\n",
    "\n",
    "clf= LinearSVC(random_state=10)\n",
    "#clf = NB()\n",
    "\n",
    "y_pred = OneVsRestClassifier(clf).fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"One-vs-All: SVC --> Mislabeled Points: %d out of %d\" % (error,xtest.shape[0]))\n",
    "\n",
    " \n",
    "clf = NB()\n",
    "\n",
    "y_pred = OneVsRestClassifier(clf).fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"One-vs-All: NB --> Mislabeled Points: %d out of %d\" % (error,xtest.shape[0]))\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "y_pred = OneVsRestClassifier(clf).fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"One-vs-All: DecisionTree --> Mislabeled Points: %d out of %d\" % (error,xtest.shape[0]))\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-vs-One: SVC --> Mislabeled Points: 100 out of 581\n",
      "One-vs-One: NB --> Mislabeled Points: 71 out of 581\n",
      "One-vs-One: DecisionTree --> Mislabeled Points: 117 out of 581\n"
     ]
    }
   ],
   "source": [
    "# One-vs-One (All-vs-All)\n",
    "\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "clf= LinearSVC(random_state=10)\n",
    "#clf = NB()\n",
    "\n",
    "y_pred = OneVsOneClassifier(clf).fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"One-vs-One: SVC --> Mislabeled Points: %d out of %d\" % (error,xtest.shape[0]))\n",
    " \n",
    "clf = NB()\n",
    "\n",
    "y_pred = OneVsOneClassifier(clf).fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"One-vs-One: NB --> Mislabeled Points: %d out of %d\" % (error,xtest.shape[0]))\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "y_pred = OneVsOneClassifier(clf).fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"One-vs-One: DecisionTree --> Mislabeled Points: %d out of %d\" % (error,xtest.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:  SVC performs best with one-vs-all classification and NB performs best with one-vs-one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the classifier process the weights of the data points to focus on misclassified data points?  \n",
    "The algorithm calculates the error the weak classifier made predicting the output variable for the training instance.  This turns out to be 0 if y from training set equals predicted y from the weak learner, and one otherwise.  This is then used in a weight adjustment formula."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
